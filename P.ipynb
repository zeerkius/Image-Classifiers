{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "842faf81-797f-422f-8b64-1ff68b50ed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5216\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preprocessing , creating fetaures with flags , then we will turn values into numpy arrays and save as pickle files\n",
    "\n",
    "Picture_Path = [r\"C:\\Users\\agboo\\Downloads\\Pnemounia Prediction\\chest_xray\\train\"]\n",
    "\n",
    "Pnemounia_Folders = [\"NORMAL\",\"PNEUMONIA\"] # we will let pnemounia be one \n",
    "\n",
    "# pnemounia icludes virus bound pnemounia and bacterial bound penmounia\n",
    "\n",
    "Data = []\n",
    "\n",
    "def normalize_array(arr, new_min, new_max):\n",
    "    # Find the minimum and maximum values of the array\n",
    "    old_min = np.min(arr)\n",
    "    old_max = np.max(arr)\n",
    "    \n",
    "    # Apply the normalization formula\n",
    "    normalized_arr = new_min + (arr - old_min) * (new_max - new_min) / (old_max - old_min)\n",
    "    return normalized_arr\n",
    "\n",
    "\n",
    "\n",
    "def Training_Data(Train,row = 250 , col = 250):\n",
    "    for i in range(2):\n",
    "        Folder = os.path.join(Picture_Path[0],Pnemounia_Folders[i])\n",
    "        for img in os.listdir(Folder):\n",
    "            try:\n",
    "                img_path = os.path.join(Folder,img)\n",
    "                new_img = cv2.imread(img_path , cv2.IMREAD_COLOR)\n",
    "                new_img = cv2.resize(new_img,(row,col))\n",
    "                Train.append([new_img,i]) # appending the array version of the img along with the flag\n",
    "            except:\n",
    "                print(\"Error - Corrupted File\")\n",
    "        # now we will create two numpy arrays and save the fetaures in them\n",
    "\n",
    "    random.shuffle(Train) # this way we are avoiding the data being biased to one value\n",
    "\n",
    "    Images = []\n",
    "    Flags = []\n",
    "\n",
    "    for key , flag in Train:\n",
    "        Images.append(normalize_array(key , 0 , 1))\n",
    "        Flags.append(flag)\n",
    "\n",
    "    Images = numpy.array(Images).reshape(-1,row,col,3) # we only need one since we are working with X-rays therfore grayscaled data\n",
    "    Flags = numpy.array(Flags)\n",
    "\n",
    "    \n",
    "    pickle_out = open(\"Images_Nov.pickle\" , \"wb\")\n",
    "    pickle.dump(Images , pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "    pickle_out = open(\"Flags_Nov.pickle\", \"wb\")\n",
    "    pickle.dump(Flags , pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "    # this ends our pre-processing for our supervised learning model.\n",
    "       \n",
    "Training_Data(Data)\n",
    "print(len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f12752-be89-48ca-8679-623e43bd50be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 273ms/step - accuracy: 0.7898 - loss: 1.2321 - val_accuracy: 0.9435 - val_loss: 0.1848\n",
      "Epoch 2/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 266ms/step - accuracy: 0.9489 - loss: 0.1402 - val_accuracy: 0.9713 - val_loss: 0.0917\n",
      "Epoch 3/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 264ms/step - accuracy: 0.9589 - loss: 0.1017 - val_accuracy: 0.9559 - val_loss: 0.1106\n",
      "Epoch 4/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 264ms/step - accuracy: 0.9713 - loss: 0.0791 - val_accuracy: 0.9722 - val_loss: 0.0716\n",
      "Epoch 5/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 264ms/step - accuracy: 0.9776 - loss: 0.0665 - val_accuracy: 0.9751 - val_loss: 0.0650\n",
      "Epoch 6/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 266ms/step - accuracy: 0.9702 - loss: 0.0661 - val_accuracy: 0.9703 - val_loss: 0.0854\n",
      "Epoch 7/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 226ms/step - accuracy: 0.9754 - loss: 0.0650 - val_accuracy: 0.9713 - val_loss: 0.0689\n",
      "Epoch 8/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 225ms/step - accuracy: 0.9834 - loss: 0.0485 - val_accuracy: 0.9703 - val_loss: 0.0727\n",
      "Epoch 9/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 223ms/step - accuracy: 0.9899 - loss: 0.0368 - val_accuracy: 0.9693 - val_loss: 0.0696\n",
      "Epoch 10/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 224ms/step - accuracy: 0.9817 - loss: 0.0444 - val_accuracy: 0.9770 - val_loss: 0.0556\n",
      "Epoch 11/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 227ms/step - accuracy: 0.9848 - loss: 0.0486 - val_accuracy: 0.9751 - val_loss: 0.0674\n",
      "Epoch 12/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 225ms/step - accuracy: 0.9928 - loss: 0.0272 - val_accuracy: 0.9741 - val_loss: 0.0638\n",
      "Epoch 13/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 226ms/step - accuracy: 0.9924 - loss: 0.0228 - val_accuracy: 0.9761 - val_loss: 0.0594\n",
      "Epoch 14/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 225ms/step - accuracy: 0.9968 - loss: 0.0164 - val_accuracy: 0.9761 - val_loss: 0.0727\n",
      "Epoch 15/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 225ms/step - accuracy: 0.9982 - loss: 0.0136 - val_accuracy: 0.9761 - val_loss: 0.0638\n",
      "Epoch 16/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 225ms/step - accuracy: 0.9978 - loss: 0.0096 - val_accuracy: 0.9703 - val_loss: 0.0825\n",
      "Epoch 17/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 226ms/step - accuracy: 0.9982 - loss: 0.0089 - val_accuracy: 0.9761 - val_loss: 0.0674\n",
      "Epoch 18/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 225ms/step - accuracy: 0.9999 - loss: 0.0055 - val_accuracy: 0.9828 - val_loss: 0.0747\n",
      "Epoch 19/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 225ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.9741 - val_loss: 0.0695\n",
      "Epoch 20/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 227ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.9818 - val_loss: 0.0748\n",
      "Epoch 21/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 224ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.9799 - val_loss: 0.0773\n",
      "Epoch 22/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 231ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.9770 - val_loss: 0.0846\n",
      "Epoch 23/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.9808 - val_loss: 0.0831\n",
      "Epoch 24/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 226ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9780 - val_loss: 0.0934\n",
      "Epoch 25/25\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 222ms/step - accuracy: 0.9976 - loss: 0.0053 - val_accuracy: 0.9761 - val_loss: 0.0784\n",
      "Epoch 1/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 608ms/step - accuracy: 0.4933 - loss: 6.4093 - val_accuracy: 0.7011 - val_loss: 0.6745\n",
      "Epoch 2/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 608ms/step - accuracy: 0.7547 - loss: 0.6607 - val_accuracy: 0.7011 - val_loss: 0.6514\n",
      "Epoch 3/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 600ms/step - accuracy: 0.7571 - loss: 0.6316 - val_accuracy: 0.7011 - val_loss: 0.6353\n",
      "Epoch 4/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 600ms/step - accuracy: 0.7458 - loss: 0.6141 - val_accuracy: 0.7011 - val_loss: 0.6242\n",
      "Epoch 5/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 604ms/step - accuracy: 0.7441 - loss: 0.5998 - val_accuracy: 0.7011 - val_loss: 0.6170\n",
      "Epoch 6/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 612ms/step - accuracy: 0.7533 - loss: 0.5835 - val_accuracy: 0.7011 - val_loss: 0.6128\n",
      "Epoch 7/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 1s/step - accuracy: 0.7417 - loss: 0.5832 - val_accuracy: 0.7011 - val_loss: 0.6106\n",
      "Epoch 8/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4399s\u001b[0m 25s/step - accuracy: 0.7526 - loss: 0.5701 - val_accuracy: 0.7011 - val_loss: 0.6099\n",
      "Epoch 9/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 708ms/step - accuracy: 0.7586 - loss: 0.5611 - val_accuracy: 0.7011 - val_loss: 0.6100\n",
      "Epoch 10/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 712ms/step - accuracy: 0.7578 - loss: 0.5591 - val_accuracy: 0.7011 - val_loss: 0.6106\n",
      "Epoch 11/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 708ms/step - accuracy: 0.7585 - loss: 0.5566 - val_accuracy: 0.7011 - val_loss: 0.6114\n",
      "Epoch 12/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 676ms/step - accuracy: 0.7528 - loss: 0.5610 - val_accuracy: 0.7011 - val_loss: 0.6122\n",
      "Epoch 13/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 682ms/step - accuracy: 0.7425 - loss: 0.5707 - val_accuracy: 0.7011 - val_loss: 0.6130\n",
      "Epoch 14/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 694ms/step - accuracy: 0.7447 - loss: 0.5682 - val_accuracy: 0.7011 - val_loss: 0.6137\n",
      "Epoch 15/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 676ms/step - accuracy: 0.7443 - loss: 0.5686 - val_accuracy: 0.7011 - val_loss: 0.6145\n",
      "Epoch 16/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 683ms/step - accuracy: 0.7412 - loss: 0.5718 - val_accuracy: 0.7011 - val_loss: 0.6148\n",
      "Epoch 17/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 680ms/step - accuracy: 0.7442 - loss: 0.5686 - val_accuracy: 0.7011 - val_loss: 0.6153\n",
      "Epoch 18/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 726ms/step - accuracy: 0.7536 - loss: 0.5584 - val_accuracy: 0.7011 - val_loss: 0.6153\n",
      "Epoch 19/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 702ms/step - accuracy: 0.7596 - loss: 0.5519 - val_accuracy: 0.7011 - val_loss: 0.6156\n",
      "Epoch 20/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 689ms/step - accuracy: 0.7495 - loss: 0.5629 - val_accuracy: 0.7011 - val_loss: 0.6156\n",
      "Epoch 21/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 681ms/step - accuracy: 0.7451 - loss: 0.5677 - val_accuracy: 0.7011 - val_loss: 0.6157\n",
      "Epoch 22/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 684ms/step - accuracy: 0.7528 - loss: 0.5592 - val_accuracy: 0.7011 - val_loss: 0.6159\n",
      "Epoch 23/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 685ms/step - accuracy: 0.7485 - loss: 0.5640 - val_accuracy: 0.7011 - val_loss: 0.6160\n",
      "Epoch 24/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 686ms/step - accuracy: 0.7491 - loss: 0.5633 - val_accuracy: 0.7011 - val_loss: 0.6161\n",
      "Epoch 25/25\n",
      "\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 699ms/step - accuracy: 0.7544 - loss: 0.5575 - val_accuracy: 0.7011 - val_loss: 0.6159\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "import numpy as np\n",
    "import collections\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Conv2D , RandomZoom , Activation , Flatten , Dense , MaxPooling2D , AveragePooling2D , Dropout\n",
    "\n",
    "# we want our model to have a toplogy with a graph like this , conider this graph to be a DAG\n",
    "edges = [[0,3],[1,3],[2,4],[3,5],[4,5],[5,6],[5,7],[6,8],[7,8]]\n",
    "\n",
    "# 3 input /conv layers \n",
    "# 2  pooling layers\n",
    "#  1 flatten layers\n",
    "#  2 dense layers\n",
    "#  1 output\n",
    "\n",
    "# we can define paths in the graph like this\n",
    "\n",
    "seen = set() # global set\n",
    "    \n",
    "        \n",
    "def builder(edges):\n",
    "    # build adgecency list\n",
    "    adj = collections.defaultdict(list)\n",
    "    for start , end in edges:\n",
    "        adj[start].append(end)\n",
    "    return adj\n",
    "\n",
    "def path_finder(adj_list,node,dst):\n",
    "    seen.add(node)\n",
    "    if node == dst:\n",
    "        return seen\n",
    "    \n",
    "    for end in adj_list[node]:\n",
    "        if end not in seen and path_finder(adj_list,end,dst):\n",
    "            return seen\n",
    "    return False\n",
    "\n",
    "# now we must open the file\n",
    "\n",
    "Images = pickle.load(open(\"Images_Nov.pickle\" ,\"rb\"))\n",
    "Flags = pickle.load(open(\"Flags_Nov.pickle\" , \"rb\"))\n",
    "\n",
    "\n",
    "       \n",
    "# tried input normalization to speed up training process but ran into issues 11/3/24\n",
    "# i think I will just train with the full shape , however this will make training time slower\n",
    "\n",
    "# creating input nodes\n",
    "# our input shape is 1250  x 800 , we specified this in the pre-processing X 3 , this is because since its a picture we are working with RGB\n",
    "\n",
    "\n",
    "# we will do this with diffrent  nodes so our values will be diffrent\n",
    "\n",
    "img_input_shape = keras.Input(Images.shape[1:])\n",
    "\n",
    "\n",
    "\n",
    "convo_layer0 = Conv2D(32 , (3,3))\n",
    "\n",
    "convo_layer1 = Conv2D(28 , (2,2))\n",
    "\n",
    "convo_layer2 = Conv2D(64 , (3,3))\n",
    "\n",
    "# then we will link the maxpooling layers \n",
    "\n",
    "pooling3 = MaxPooling2D(pool_size = (3,3))\n",
    "\n",
    "pooling4 = MaxPooling2D(pool_size = (2,2))\n",
    "\n",
    "\n",
    "# first conv - > pooling\n",
    "\n",
    "A = convo_layer0(img_input_shape)\n",
    "a = pooling3(A)\n",
    "\n",
    "B = convo_layer1(img_input_shape)\n",
    "b = pooling3(B)\n",
    "\n",
    "C = convo_layer2(img_input_shape)\n",
    "c = pooling4(C)\n",
    "\n",
    "# Flatten Layer\n",
    "\n",
    "flatten5 = Flatten()\n",
    "\n",
    "#  pooling - > flatten\n",
    "\n",
    "d = flatten5(b)\n",
    "e = flatten5(c)\n",
    "\n",
    "# Dense \n",
    "\n",
    "Dense6 = Dense(48 , activation = \"relu\" , use_bias = False)\n",
    "\n",
    "Dense7 = Dense(48 , activation = \"relu\" , use_bias = True)\n",
    "\n",
    "# flatten -> Dense\n",
    "\n",
    "f = Dense6(d)\n",
    "g = Dense7(e)\n",
    "\n",
    "\n",
    "# Technical Output Layer\n",
    "\n",
    "Dense8 = Dense(1 , activation = \"sigmoid\") # since this a binary classification\n",
    "\n",
    "# dense - > Output\n",
    "\n",
    "h = Dense8(f)\n",
    "i = Dense8(g)\n",
    "\n",
    "\n",
    "# we will then form all the paths \n",
    "# this will give us 6 distinct paths\n",
    "\n",
    "\n",
    "model_one = keras.Model(inputs = img_input_shape , outputs = h , name = \"Model1\")\n",
    "model_two = keras.Model(inputs = img_input_shape , outputs = i , name = \"Model2\")\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "Board_Name = \"Comparison\"\n",
    "\n",
    "tensor = TensorBoard(log_dir = 'logs/{}'.format(Board_Name))\n",
    "\n",
    "model_one.compile(loss = \"binary_crossentropy\" , optimizer = \"adam\" , metrics = [\"accuracy\"]) # compile both models\n",
    "model_two.compile(loss = \"binary_crossentropy\" , optimizer = \"adam\" , metrics = [\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "model_one.fit(Images , Flags , batch_size = 25 , epochs = 25 , validation_split = 0.2 , callbacks = [tensor])\n",
    "model_two.fit(Images , Flags, batch_size = 25 , epochs = 25 , validation_split = 0.15 , callbacks = [tensor])\n",
    "\n",
    "model_one.save(\"Model_1.keras\")\n",
    "model_two.save(\"Model_2.keras\")\n",
    "\n",
    "# Model one is alot more accurate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb30a1e-5a60-482d-aa3b-ee3c939f9d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
